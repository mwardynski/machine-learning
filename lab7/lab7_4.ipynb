{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardy≈Ñski  \n",
    "wtorek, 9:45\n",
    "\n",
    "## Laboratorium 7\n",
    "### 7.4 AE\n",
    "\n",
    "\n",
    "#### Funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import importlib\n",
    "import lab7_utils as utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FrozenEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "    \n",
    "    \n",
    "def viz_accuracy_to_latent_dim(latent_dims, scores, score_name):\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(latent_dims, scores)\n",
    "    plt.title(f\"{score_name} - Lattent Layer Dimension\")\n",
    "    plt.xlabel(\"Lattent Layer Dimension\")\n",
    "    plt.ylabel(f\"{score_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(len(latent_dims)):\n",
    "        print(f\"[{latent_dims[i]}: {scores[i]:.4f}]\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate_autoencoder(dataset_name, params, create_ae_fun):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = utils.get_dataset_for_ae(dataset_name=dataset_name, with_val=False)\n",
    "\n",
    "    keras_reg = KerasRegressor(\n",
    "        model=create_ae_fun,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        model__latent_dim=128,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    grid_search = GridSearchCV(keras_reg, params, cv=7, n_jobs=-1)\n",
    "    grid_search.fit(X_train, X_train)\n",
    "\n",
    "    r2_scores = np.array(grid_search.cv_results_['mean_test_score'])\n",
    "    latent_dims = np.array(list(map(lambda t: t['model__latent_dim'], grid_search.cv_results_['params'])))\n",
    "    \n",
    "    \n",
    "    viz_accuracy_to_latent_dim(latent_dims, r2_scores, \"R2\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def create_model_instance(class_name, *args, **kwargs):\n",
    "    return class_name(*args, **kwargs)\n",
    "\n",
    "def find_best_encoder(dataset_name, model_name, latent_dims):\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.get_dataset_for_ae(dataset_name, with_val=True)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    best_latent_dim = 0\n",
    "    best_model = None\n",
    "    accuracies = []\n",
    "    for latent_dim in latent_dims:\n",
    "        model = create_model_instance(model_name, latent_dim)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), loss=keras.losses.BinaryCrossentropy())\n",
    "        model.fit(X_train, X_train,\n",
    "                epochs=40,\n",
    "                shuffle=True,\n",
    "                batch_size=64,\n",
    "                verbose=0,\n",
    "                validation_data=(X_val, X_val))\n",
    "        \n",
    "        frozen_model = FrozenEncoder(encoder=model.encoder)\n",
    "\n",
    "        pipeline_log_reg = Pipeline([\n",
    "            ('frozen_model', frozen_model),\n",
    "            ('log_reg', LogisticRegression(max_iter=1000, solver=\"newton-cg\"))\n",
    "        ])\n",
    "\n",
    "        pipeline_log_reg.fit(X_train, y_train)\n",
    "        y_pred = pipeline_log_reg.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if max_accuracy < accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_latent_dim = latent_dim\n",
    "            best_model = frozen_model\n",
    "    \n",
    "    viz_accuracy_to_latent_dim(latent_dims, accuracies, \"Accuracy\")\n",
    "    return best_model, best_latent_dim\n",
    "\n",
    "\n",
    "def classify_with_encoder(dataset_name, encoder_model, head_model):\n",
    "    X_train, X_test, y_train, y_test = utils.get_dataset_for_ae(dataset_name, with_val=False)\n",
    "    \n",
    "    pipeline_log_reg = Pipeline([\n",
    "        ('encoder', encoder_model),\n",
    "        ('head', head_model)\n",
    "    ])\n",
    "\n",
    "    pipeline_log_reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline_log_reg.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(SimpleAutoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.mid_dim = 384\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(shape=(784, )),\n",
    "            tf.keras.layers.Dense(self.mid_dim, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(latent_dim, activation=tf.nn.relu),\n",
    "        ]\n",
    "    )\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(self.mid_dim, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(units=784, activation=tf.nn.sigmoid),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded    \n",
    "\n",
    "def create_simple_ae_as_sequence(latent_dim):\n",
    "  model =  SimpleAutoencoder(latent_dim)\n",
    "  return tf.keras.Sequential([model.encoder, model.decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = [10, 64, 128, 256, 512]\n",
    "\n",
    "param_grid = {\n",
    "    'model__latent_dim': latent_dims,\n",
    "    'epochs': [40]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.MNIST.value, param_grid, create_simple_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.MNIST.value, SimpleAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Simple Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Simple Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.F_MNIST.value, param_grid, create_simple_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.F_MNIST.value, SimpleAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Simple Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, f_mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Simple Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, f_mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kuzushiji MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.K_MNIST.value, param_grid, create_simple_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.K_MNIST.value, SimpleAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Simple Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, k_mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Simple Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, k_mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(ConvAutoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=(784,)),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu, padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(2, padding='same'),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu, padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(2, padding='same'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(latent_dim),\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape((7, 7, 32)),\n",
    "            tf.keras.layers.Conv2DTranspose(64, 3, 2, activation=tf.nn.relu, padding='same'),\n",
    "            tf.keras.layers.Conv2DTranspose(32, 3, 2, activation=tf.nn.relu, padding='same'),\n",
    "            tf.keras.layers.Conv2D(1, (3, 3), activation=tf.nn.sigmoid, padding='same'),\n",
    "            tf.keras.layers.Reshape(target_shape=(784,))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "  def create_conv_ae_as_sequence(latent_dim):\n",
    "    model =  ConvAutoencoder(latent_dim)\n",
    "    return tf.keras.Sequential([model.encoder, model.decoder])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.MNIST.value, param_grid, create_conv_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.MNIST.value, ConvAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Convolutional Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Convolutional Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.F_MNIST.value, param_grid, create_conv_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.F_MNIST.value, ConvAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Convolutional Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, f_mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Convolutional Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, f_mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kuzushiji MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoder(utils.Dataset_Select.K_MNIST.value, param_grid, create_conv_ae_as_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_mnist_encoder_model, latent_dim = find_best_encoder(utils.Dataset_Select.K_MNIST.value, ConvAutoencoder, latent_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = LogisticRegression(max_iter=1000, solver=\"newton-cg\")\n",
    "print(\"\\n--- Convolutional Autoencoder with Logistic Regression ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, k_mnist_encoder_model, head_model)\n",
    "\n",
    "head_model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "print(\"\\n\\n--- Convolutional Autoencoder with Random Forest ---\\n\")\n",
    "classify_with_encoder(utils.Dataset_Select.F_MNIST.value, k_mnist_encoder_model, head_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent 256:\n",
    "\n",
    "==== LogisticRegression (ConvAE) ====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.85      0.84      1000\n",
    "           1       0.99      0.98      0.98      1000\n",
    "           2       0.80      0.83      0.82      1000\n",
    "           3       0.88      0.91      0.90      1000\n",
    "           4       0.80      0.81      0.81      1000\n",
    "           5       0.97      0.97      0.97      1000\n",
    "           6       0.72      0.66      0.69      1000\n",
    "           7       0.95      0.97      0.96      1000\n",
    "           8       0.97      0.97      0.97      1000\n",
    "           9       0.97      0.96      0.97      1000\n",
    "\n",
    "    accuracy                           0.89     10000\n",
    "   macro avg       0.89      0.89      0.89     10000\n",
    "weighted avg       0.89      0.89      0.89     10000\n",
    "\n",
    "Accuracy (ConvAE + LogisticRegression): 0.8900\n",
    "\n",
    "Latent 10:\n",
    "\n",
    "==== LogisticRegression (ConvAE) ====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.78      0.76      1000\n",
    "           1       0.96      0.94      0.95      1000\n",
    "           2       0.64      0.60      0.62      1000\n",
    "           3       0.76      0.84      0.80      1000\n",
    "           4       0.61      0.68      0.64      1000\n",
    "           5       0.91      0.89      0.90      1000\n",
    "           6       0.45      0.34      0.39      1000\n",
    "           7       0.87      0.89      0.88      1000\n",
    "           8       0.92      0.94      0.93      1000\n",
    "           9       0.91      0.92      0.92      1000\n",
    "\n",
    "    accuracy                           0.78     10000\n",
    "   macro avg       0.78      0.78      0.78     10000\n",
    "weighted avg       0.78      0.78      0.78     10000\n",
    "\n",
    "Accuracy (ConvAE + LogisticRegression): 0.7834\n",
    "\n",
    "Dense Latent 512:\n",
    "\n",
    "```\n",
    "==== LogisticRegression (ConvAE) ====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.85      0.84      1000\n",
    "           1       0.99      0.98      0.98      1000\n",
    "           2       0.83      0.85      0.84      1000\n",
    "           3       0.89      0.91      0.90      1000\n",
    "           4       0.82      0.83      0.83      1000\n",
    "           5       0.97      0.97      0.97      1000\n",
    "           6       0.74      0.68      0.71      1000\n",
    "           7       0.95      0.97      0.96      1000\n",
    "           8       0.97      0.98      0.98      1000\n",
    "           9       0.98      0.96      0.97      1000\n",
    "\n",
    "    accuracy                           0.90     10000\n",
    "   macro avg       0.90      0.90      0.90     10000\n",
    "weighted avg       0.90      0.90      0.90     10000\n",
    "\n",
    "Accuracy (ConvAE + LogisticRegression): 0.8979\n",
    "```\n",
    "\n",
    "\n",
    "No mod:\n",
    "\n",
    "```\n",
    "==== LogisticRegression (ConvAE) ====\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.87      0.85      1000\n",
    "           1       0.99      0.98      0.99      1000\n",
    "           2       0.85      0.85      0.85      1000\n",
    "           3       0.90      0.91      0.91      1000\n",
    "           4       0.84      0.86      0.85      1000\n",
    "           5       0.98      0.98      0.98      1000\n",
    "           6       0.77      0.72      0.74      1000\n",
    "           7       0.96      0.98      0.97      1000\n",
    "           8       0.98      0.97      0.98      1000\n",
    "           9       0.98      0.97      0.97      1000\n",
    "\n",
    "    accuracy                           0.91     10000\n",
    "   macro avg       0.91      0.91      0.91     10000\n",
    "weighted avg       0.91      0.91      0.91     10000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae = ConvAutoencoder(256)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "conv_ae.compile(optimizer=optimizer, loss=keras.losses.BinaryCrossentropy())\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utils.get_dataset_for_ae(utils.Dataset_Select.F_MNIST.value, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 38ms/step - loss: 0.3652 - val_loss: 0.2757\n",
      "Epoch 2/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - loss: 0.2726 - val_loss: 0.2672\n",
      "Epoch 3/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - loss: 0.2653 - val_loss: 0.2635\n",
      "Epoch 4/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2625 - val_loss: 0.2612\n",
      "Epoch 5/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2604 - val_loss: 0.2598\n",
      "Epoch 6/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2595 - val_loss: 0.2588\n",
      "Epoch 7/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - loss: 0.2574 - val_loss: 0.2583\n",
      "Epoch 8/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2572 - val_loss: 0.2571\n",
      "Epoch 9/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2560 - val_loss: 0.2566\n",
      "Epoch 10/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2554 - val_loss: 0.2563\n",
      "Epoch 11/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2548 - val_loss: 0.2557\n",
      "Epoch 12/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2551 - val_loss: 0.2552\n",
      "Epoch 13/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - loss: 0.2540 - val_loss: 0.2550\n",
      "Epoch 14/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - loss: 0.2546 - val_loss: 0.2545\n",
      "Epoch 15/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - loss: 0.2541 - val_loss: 0.2542\n",
      "Epoch 16/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2530 - val_loss: 0.2540\n",
      "Epoch 17/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - loss: 0.2536 - val_loss: 0.2539\n",
      "Epoch 18/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - loss: 0.2526 - val_loss: 0.2536\n",
      "Epoch 19/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - loss: 0.2520 - val_loss: 0.2534\n",
      "Epoch 20/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2526 - val_loss: 0.2531\n",
      "Epoch 21/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - loss: 0.2520 - val_loss: 0.2530\n",
      "Epoch 22/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2522 - val_loss: 0.2528\n",
      "Epoch 23/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2519 - val_loss: 0.2526\n",
      "Epoch 24/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2514 - val_loss: 0.2527\n",
      "Epoch 25/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - loss: 0.2510 - val_loss: 0.2523\n",
      "Epoch 26/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2518 - val_loss: 0.2522\n",
      "Epoch 27/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 38ms/step - loss: 0.2516 - val_loss: 0.2522\n",
      "Epoch 28/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 38ms/step - loss: 0.2516 - val_loss: 0.2519\n",
      "Epoch 29/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2511 - val_loss: 0.2519\n",
      "Epoch 30/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - loss: 0.2507 - val_loss: 0.2517\n",
      "Epoch 31/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2514 - val_loss: 0.2516\n",
      "Epoch 32/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2510 - val_loss: 0.2517\n",
      "Epoch 33/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - loss: 0.2502 - val_loss: 0.2514\n",
      "Epoch 34/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 39ms/step - loss: 0.2510 - val_loss: 0.2514\n",
      "Epoch 35/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - loss: 0.2502 - val_loss: 0.2514\n",
      "Epoch 36/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 41ms/step - loss: 0.2497 - val_loss: 0.2513\n",
      "Epoch 37/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - loss: 0.2498 - val_loss: 0.2511\n",
      "Epoch 38/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 41ms/step - loss: 0.2499 - val_loss: 0.2511\n",
      "Epoch 39/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - loss: 0.2506 - val_loss: 0.2513\n",
      "Epoch 40/40\n",
      "\u001b[1m804/804\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - loss: 0.2503 - val_loss: 0.2510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x133d22ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_ae.fit(X_train, X_train,\n",
    "                epochs=40,\n",
    "                shuffle=True,\n",
    "                batch_size=64,\n",
    "                validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FrozenEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.encoder.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.82      0.83      0.82      1000\n",
      "           3       0.88      0.91      0.90      1000\n",
      "           4       0.80      0.82      0.81      1000\n",
      "           5       0.97      0.97      0.97      1000\n",
      "           6       0.70      0.64      0.67      1000\n",
      "           7       0.94      0.96      0.95      1000\n",
      "           8       0.97      0.97      0.97      1000\n",
      "           9       0.97      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "frozen_conv_ae = FrozenEncoder(encoder=conv_ae.encoder)\n",
    "\n",
    "X_train, X_test, y_train, y_test = utils.get_dataset_for_ae(dataset_name=utils.Dataset_Select.F_MNIST.value, with_val=False)\n",
    "\n",
    "pipeline_log_reg = Pipeline([\n",
    "    ('frozen_conv_ae', frozen_conv_ae),\n",
    "    ('log_reg', LogisticRegression(max_iter=5000, solver=\"newton-cg\"))\n",
    "])\n",
    "\n",
    "pipeline_log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline_log_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3(256)\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.83      0.83      1000\n",
    "           1       0.98      0.98      0.98      1000\n",
    "           2       0.81      0.82      0.82      1000\n",
    "           3       0.88      0.91      0.89      1000\n",
    "           4       0.80      0.82      0.81      1000\n",
    "           5       0.97      0.97      0.97      1000\n",
    "           6       0.69      0.64      0.67      1000\n",
    "           7       0.94      0.97      0.95      1000\n",
    "           8       0.97      0.97      0.97      1000\n",
    "           9       0.97      0.96      0.97      1000\n",
    "\n",
    "    accuracy                           0.89     10000\n",
    "   macro avg       0.89      0.89      0.89     10000\n",
    "weighted avg       0.89      0.89      0.89     10000\n",
    "```\n",
    "1(256)\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.82      0.82      1000\n",
    "           1       0.98      0.96      0.97      1000\n",
    "           2       0.75      0.77      0.76      1000\n",
    "           3       0.85      0.89      0.87      1000\n",
    "           4       0.76      0.77      0.76      1000\n",
    "           5       0.96      0.96      0.96      1000\n",
    "           6       0.65      0.59      0.62      1000\n",
    "           7       0.93      0.95      0.94      1000\n",
    "           8       0.96      0.96      0.96      1000\n",
    "           9       0.96      0.96      0.96      1000\n",
    "\n",
    "    accuracy                           0.86     10000\n",
    "   macro avg       0.86      0.86      0.86     10000\n",
    "weighted avg       0.86      0.86      0.86     10000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate_conv_autoencoder(dataset_name, params, create_plain_ae_fun, create_ae_from_params_fun):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = utils.get_dataset_for_ae(dataset_name=dataset_name, with_val=False)\n",
    "\n",
    "    keras_reg = KerasRegressor(\n",
    "        model=create_plain_ae_fun,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        optimizer__learning_rate=0.001,\n",
    "        model__latent_dim=10,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    grid_search = GridSearchCV(keras_reg, params, refit=False, cv=7, n_jobs=-1)\n",
    "    grid_search.fit(X_train, X_train)\n",
    "\n",
    "    print(grid_search.best_score_, grid_search.best_params_)\n",
    "\n",
    "    X_train_s, X_val_s, _, _, _, _ = utils.get_dataset_for_ae(dataset_name=dataset_name, with_val=True)\n",
    "\n",
    "    ae = create_ae_from_params_fun(grid_search.best_params_)\n",
    "    ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=grid_search.best_params_[\"optimizer__learning_rate\"]), loss=keras.losses.BinaryCrossentropy)\n",
    "    ae.fit(X_train_s, X_train_s,\n",
    "                epochs=grid_search.best_params_[\"epochs\"],\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val_s, X_val_s))\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "2025-01-03 07:21:01.541260: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0] vs. [6272,384]\n",
      "\t [[{{function_node __inference_one_step_on_data_202248}}{{node adam/truediv_13}}]]\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*urllib3.*LibreSSL.*\")\n",
    "\n",
    "param_grid = {\n",
    "    'model__latent_dim': [10, 64, 128, 192],\n",
    "    'optimizer__learning_rate': [0.0001, 0.005, 0.001],\n",
    "    'epochs': [40]\n",
    "}\n",
    "\n",
    "evaluated_conv_ae = evaluate_conv_autoencoder(utils.Dataset_Select.F_MNIST.value, param_grid, create_conv_ae_as_sequence, create_conv_ae_from_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "Nieudane pr√≥by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "  def __init__(self, latent_dim):\n",
    "    super(CVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(shape=(784)),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(7, 7, 64)),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                activation='relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                activation='relu'),\n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.Reshape(target_shape=(784)),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = self.prepare_encoder()\n",
    "        self.decoder = self.prepare_decoder()\n",
    "\n",
    "    def prepare_encoder(self):\n",
    "        input_img = tf.keras.layers.Input(shape=(784,))\n",
    "        x = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(input_img)\n",
    "        x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(self.latent_dim + self.latent_dim)(x)\n",
    "        z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        return  tf.keras.Model([input_img], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    def prepare_decoder(self):\n",
    "        input = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
    "        x = tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu)(input)\n",
    "        x = tf.keras.layers.Reshape(target_shape=(7, 7, 64))(x)\n",
    "        x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same',activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same',activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation=tf.nn.sigmoid)(x) \n",
    "        x = tf.keras.layers.Reshape(target_shape=(784,))(x)\n",
    "        return  tf.keras.Model([input], [x], name=\"decoder\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        self.add_loss(self.kl_loss(z_mean, z_log_var))\n",
    "        return reconstructed\n",
    "\n",
    "    def kl_loss(self, z_mean, z_log_var):\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        return tf.reduce_mean(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sampling_layer = Sampling()\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(shape=(784,)),\n",
    "                tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "                tf.keras.layers.Reshape(target_shape=(784,))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = tf.split(self.encoder(inputs), num_or_size_splits=2, axis=1)\n",
    "        z = self.sampling_layer([z_mean, z_log_var])\n",
    "        reconstructed = self.decoder(z)\n",
    "        self.add_loss(self.kl_loss(z_mean, z_log_var))\n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "    def kl_loss(self, z_mean, z_log_var):\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        return tf.reduce_mean(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(64)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "vae.compile(optimizer=optimizer, loss=keras.losses.BinaryCrossentropy())\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = utils.get_dataset_for_ae(utils.Dataset_Select.MNIST.value, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1, 784)\n",
    "# X_val = X_val.reshape(-1, 784)\n",
    "\n",
    "vae.fit(X_train, X_train,\n",
    "                epochs=3,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FrozenVEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        z_mean, z_log_var = tf.split(self.encoder.predict(X), num_or_size_splits=2, axis=1)\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "frozen_vencoder = FrozenEncoder(encoder=vae.encoder)\n",
    "\n",
    "X_train, X_test, y_train, y_test = utils.get_dataset_for_ae(dataset_name=utils.Dataset_Select.MNIST.value, with_val=False)\n",
    "\n",
    "pipeline_log_reg = Pipeline([\n",
    "    ('frozen_vencoder', frozen_vencoder),\n",
    "    ('log_reg', LogisticRegression(max_iter=2000, solver=\"newton-cg\"))\n",
    "])\n",
    "\n",
    "pipeline_log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline_log_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
