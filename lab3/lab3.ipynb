{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "      \n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.probs = None\n",
    "      \n",
    "    def fit(self, X, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        self.probs = np.zeros(len(values))\n",
    "        self.mean = np.zeros((len(values), X.shape[1]))\n",
    "        self.std = np.zeros((len(values), X.shape[1]))\n",
    "        \n",
    "        for i, value in enumerate(values):\n",
    "            self.probs[value] = counts[i]/len(y)\n",
    "\n",
    "        for label in range(len(self.probs)):\n",
    "            indices = np.where(y == label)[0]\n",
    "            relevant_samples = X[indices]\n",
    "            for feat in range(relevant_samples.shape[1]):\n",
    "                feat_mean = np.mean(relevant_samples[:, feat])\n",
    "                feat_std = np.std(relevant_samples[:, feat])\n",
    "              \n",
    "                self.mean[label][feat] = feat_mean\n",
    "                self.std[label][feat] = feat_std       \n",
    "        return self\n",
    "      \n",
    "    def predict(self, X):\n",
    "        labels = []\n",
    "        for x in X:\n",
    "            selected_k = -1\n",
    "            max_prob_for_k = 0\n",
    "            for k in range(len(self.probs)):\n",
    "                sum = 0\n",
    "                for i in range(self.mean.shape[1]):\n",
    "                    prob_xi_ck = 1/math.sqrt(2*math.pi*self.std[k][i])*math.exp(-((x[i]-self.mean[k][i])**2)/(2*self.std[k][i]))\n",
    "                    sum += prob_xi_ck\n",
    "                prob_k = math.log(self.probs[k]) + sum\n",
    "                if(max_prob_for_k < prob_k):\n",
    "                    max_prob_for_k = prob_k\n",
    "                    selected_k = k\n",
    "            labels.append(selected_k)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg-values -> precision: 0.946031746031746, recall: 0.9333333333333333, f1 score: 0.9333333333333333\n",
      "avg-values -> precision: 0.9575, recall: 0.95, f1 score: 0.9501039501039501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "my_gnb = GaussianNaiveBayes()\n",
    "my_gnb.fit(X_train, y_train)\n",
    "y_pred = my_gnb.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BernoulliNaiveBayes:\n",
    "      \n",
    "#     def __init__(self):\n",
    "#         self.probs = None\n",
    "#         self.feats = None\n",
    "      \n",
    "#     def fit(self, X, y):\n",
    "#         values, counts = np.unique(y, return_counts=True)\n",
    "#         self.probs = counts/len(y)\n",
    "        \n",
    "#         self.feats = np.zeros((len(self.probs), X.shape[1]))\n",
    "#         for i, k in enumerate(values):\n",
    "#             indices = np.where(y == k)\n",
    "#             relevant_samples = X[indices]\n",
    "#             self.feats[i, :] = (np.sum(relevant_samples, axis=0) + 1) / (relevant_samples.shape[0] + 2)\n",
    "\n",
    "        \n",
    "#         return self\n",
    "      \n",
    "#     def predict(self, X):\n",
    "#         labels = []\n",
    "\n",
    "#         for x in X:\n",
    "#             x = x.toarray().flatten()\n",
    "#             selected_k = -1\n",
    "#             max_prob_for_k = 0\n",
    "#             for k in range(len(self.probs)):\n",
    "#                 sum = 0\n",
    "#                 for i in range(x.shape[0]):\n",
    "#                     prob_xi_ck = self.feats[k][i]**x[i]*(1-self.feats[k][i])**(1-x[i])\n",
    "#                     sum += prob_xi_ck\n",
    "#                 prob_k = math.log(self.probs[k]) + sum\n",
    "#                 if(max_prob_for_k < prob_k):\n",
    "#                     max_prob_for_k = prob_k\n",
    "#                     selected_k = k\n",
    "#             labels.append(selected_k)\n",
    "\n",
    "#         return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BernoulliNaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.probs = None\n",
    "        self.feats = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        self.probs = counts / len(y)\n",
    "        self.feats = np.zeros((len(self.probs), X.shape[1]))\n",
    "\n",
    "        for i, k in enumerate(values):\n",
    "            indices = np.where(y == k)\n",
    "            relevant_samples = X[indices]\n",
    "            \n",
    "            sum = (np.sum(relevant_samples, axis=0) + 1) # +1 for Laplace smoothing\n",
    "            self.feats[i, :] = sum / (relevant_samples.shape[0] + 2) # +2 for Laplace smoothing\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, calc_logs=True):\n",
    "        labels = []\n",
    "\n",
    "        X = X.toarray().flatten()\n",
    "        for x in X:\n",
    "            # x = x_sparse.toarray().flatten()\n",
    "            selected_k = -1\n",
    "            max_prob_for_k = -1\n",
    "\n",
    "            for k in range(len(self.probs)):\n",
    "\n",
    "                prob_k = self.calculate_prob_logs(k, x) if calc_logs else self.calculate_prob_directly(k, x)\n",
    "\n",
    "                if prob_k > max_prob_for_k:\n",
    "                    max_prob_for_k = prob_k\n",
    "                    selected_k = k\n",
    "\n",
    "            labels.append(selected_k)\n",
    "\n",
    "        return labels\n",
    "    \n",
    "    def calculate_prob_directly(self, k, x):\n",
    "        prob_xi_ck = self.feats[k] ** x * (1 - self.feats[k]) ** (1 - x)\n",
    "        prob_k = self.probs[k] * np.prod(prob_xi_ck)\n",
    "        return prob_k\n",
    "    \n",
    "    def calculate_prob_logs(self, k, x):\n",
    "        log_likelihood = x * np.log(self.feats[k]) + (1 - x) * np.log(1 - self.feats[k])  \n",
    "        log_prob_k = np.log(self.probs[k]) + np.sum(log_likelihood)\n",
    "        return log_prob_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_data = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_data)\n",
    "X = cnt_vect.transform(X_data)\n",
    "X.data[X.data>1] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg-values -> precision: 0.6604541322202436, recall: 0.4917771883289125, f1 score: 0.46683201136660324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg-values -> precision: 0.6604541322202436, recall: 0.4917771883289125, f1 score: 0.46683201136660324\n"
     ]
    }
   ],
   "source": [
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg-values -> precision: 0.5756197639939095, recall: 0.39283819628647215, f1 score: 0.3815098191772453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mwardynski/Documents/ds/_semestr_9/uczenie_maszynowe/labs/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNaiveBayes()\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test, calc_logs=False)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNaiveBayes()\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BernoulliNaiveBayesGoodLogs:\n",
    "      \n",
    "    def __init__(self):\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "      \n",
    "    def fit(self, X, y):\n",
    "        # Get the unique class values and their counts\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Compute the log of class priors\n",
    "        self.class_log_prior_ = np.log(counts / len(y))  # Log of prior probabilities\n",
    "        \n",
    "        # Initialize feature log probabilities\n",
    "        self.feature_log_prob_ = np.zeros((len(self.class_log_prior_), X.shape[1]))\n",
    "\n",
    "        for i, k in enumerate(values):\n",
    "            indices = np.where(y == k)\n",
    "            relevant_samples = X[indices]\n",
    "            \n",
    "            # Laplace smoothing: Add 1 to feature count, add 2 to denominator for smoothing\n",
    "            smoothed_counts = (np.sum(relevant_samples, axis=0) + 1)\n",
    "            self.feature_log_prob_[i, :] = np.log(smoothed_counts / (relevant_samples.shape[0] + 2))\n",
    "        \n",
    "        return self\n",
    "      \n",
    "    def predict(self, X):\n",
    "        labels = []\n",
    "\n",
    "        for x in X:\n",
    "            x_dense = x.toarray().flatten()  # Convert to dense array\n",
    "            selected_k = -1\n",
    "            max_log_prob = -np.inf  # Use negative infinity for log probabilities\n",
    "\n",
    "            for k in range(len(self.class_log_prior_)):\n",
    "                # Calculate log likelihood for each class k\n",
    "                log_likelihood = x_dense * self.feature_log_prob_[k] + (1 - x_dense) * np.log(1 - np.exp(self.feature_log_prob_[k]))\n",
    "                \n",
    "                # Sum log likelihood and class prior (log of prior probability)\n",
    "                log_prob_k = self.class_log_prior_[k] + np.sum(log_likelihood)\n",
    "\n",
    "                # Select the class with the highest log probability\n",
    "                if log_prob_k > max_log_prob:\n",
    "                    max_log_prob = log_prob_k\n",
    "                    selected_k = k\n",
    "\n",
    "            labels.append(selected_k)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnbgl = BernoulliNaiveBayesGoodLogs()\n",
    "\n",
    "bnbgl.fit(X_train, y_train)\n",
    "y_pred = bnbgl.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test, calc_logs=False)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Breast Cancer Wisconsin dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Get the features and target\n",
    "X = cancer.data  # Feature matrix\n",
    "y = cancer.target  # Target labels (0 = malignant, 1 = benign)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "my_gnb = GaussianNaiveBayes()\n",
    "my_gnb.fit(X_train, y_train)\n",
    "y_pred = my_gnb.predict(X_test)\n",
    "precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f'avg-values -> precision: {precision_avg}, recall: {recall_avg}, f1 score: {f1_score_avg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
