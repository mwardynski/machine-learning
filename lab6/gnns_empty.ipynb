{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modele generatywne"
      ],
      "metadata": {
        "id": "8dW1uoIGs-kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym notebooku pobawimy się autoenkoderem, autoenkoderem wariacyjnym oraz GANem na zbiorze MNIST. Spróbujemy przygotować modele, które będą w stanie generować obrazy z zadanymi przez nas cechami (np. wybrane cyfry w określonym miejscu na obrazie).  \n",
        "  \n",
        "Do opracowania są 4 części: AE, VAE, conditioned VAE, conditioned GAN. Niektóre fragmenty wymagają samodzielnej implementacji.  \n",
        "  \n",
        "Notebook da się rozwiązać z wykorzystaniem colaba.  \n",
        "  \n",
        "Jako raport proszę przygotować plik pdf z odpowiedziami na zadania (są wyszczególnione w punktach), załączając przygotowane fragmenty kodu, opisy, wykresy - wszystko, co jest niezbędne do udokumentowania zrobienia zadania. Dodatkowo proszę załączyć notebook, w którym zawarta jest Państwa praca. Zwracam uwagę, że podstawą do oceny zadania jest <u>raport tekstowy</u> - w szczególności nie będę sprawdzać bezpośrednio notebooków (ale wymagam ich załączenia w celu potencjalnej weryfikacji tego, co jest opisane w raporcie). Oznacza to również, że notebook przekonwertowany do pdfa nie stanowi odpowiednio przygotowanego raportu.\n",
        "  \n",
        "Polecam tutorial dot. VAE: https://arxiv.org/pdf/1606.05908"
      ],
      "metadata": {
        "id": "2eyiezB2d-7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importy:"
      ],
      "metadata": {
        "id": "WSKrpun-s8Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_docs"
      ],
      "metadata": {
        "id": "Txoap-xcawVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E069-kSwl8Cz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import PIL\n",
        "import imageio.v2 as imageio\n",
        "import glob\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "ObWfqEZ-nUG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import tensorflow_docs.vis.embed as embed"
      ],
      "metadata": {
        "id": "HVFQVyh9sstK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ładowanie danych:"
      ],
      "metadata": {
        "id": "JQHLuYHDs5xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_raw, y_train_raw), (x_test_raw, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "151VnYw-mPPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. AutoEnkoder (AE)"
      ],
      "metadata": {
        "id": "oAddSm5Cxv6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zajmiemy się prostym zbiorem danych MNIST, na którym pokażemy działanie wybranych modeli generatywnych.  \n",
        "Zacznijmy od preprocessingu danych - normalizacji do przedziału [0; 1]:"
      ],
      "metadata": {
        "id": "hvCg1ttttE8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images_ae(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return images.astype('float32')\n",
        "\n",
        "x_train = preprocess_images_ae(x_train_raw)\n",
        "x_test = preprocess_images_ae(x_test_raw)"
      ],
      "metadata": {
        "id": "9BiuHraJXCAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_raw, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "99jfB1ASXI13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "7xAJ0xVpXMYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = x_train.shape[0]\n",
        "test_size = x_test.shape[0]\n",
        "val_size = x_val.shape[0]\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "kTXMU9SqXRAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotujmy datasety:"
      ],
      "metadata": {
        "id": "e33WZ5kZtgQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
        "                 .shuffle(train_size).batch(batch_size))\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices(x_val)\n",
        "               .shuffle(val_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "metadata": {
        "id": "5SKPG87oXUF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W poniższej komórce znajduje się definicja autoenkodera przygotowanego pod kątem zbioru MNIST. Zapoznaj się z architekturą sieci, składającej się z enkodera oraz dekodera."
      ],
      "metadata": {
        "id": "Q_X4uo-2tjGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(latent_dim),\n",
        "        ]\n",
        "    )\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same',\n",
        "                activation='sigmoid'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "VssHeT--xtXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 1.1 - Dlaczego sigmoid jest odpowiednią funkcją aktywacji w ostatniej warstwie dekodera w tym przypadku? (0.25pkt)"
      ],
      "metadata": {
        "id": "VhmxbTfQF3tQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadziałamy dość ekstremalnie, ustalając rozmiar przestrzeni ukrytej na 2. Zrobimy to w celu jej późniejszej wizualizacji (można by ustalić rozmiar na większy, a następnie zredukować wymiarowość przy pomocy innych metod, ale tym razem nie będziemy z tego korzystać):"
      ],
      "metadata": {
        "id": "-wgyisAQtxeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2\n",
        "autoencoder = Autoencoder(latent_dim)"
      ],
      "metadata": {
        "id": "iJAg_YhWyLQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 1.2. Skompiluj model. W tym celu najpierw zdefiniuj loss dla modelu. W przypadku autoenkodera jest to funkcja działająca na wejściach do enkodera oraz wyjściach z dekodera. Do wyboru są różne funkcje! Patrząc na reprezentację danych (wróć do funkcji definiującej preprocessing), wybierz odpowiednią. Uzasadnij swój wybór. (0.25 pkt)"
      ],
      "metadata": {
        "id": "s1Wqp18LulcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss=...)"
      ],
      "metadata": {
        "id": "5W5M4JFAyP65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Czas na trening:"
      ],
      "metadata": {
        "id": "pSDyuOKvvKP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_val, x_val))"
      ],
      "metadata": {
        "id": "1-h0D_EkyYJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wyświetlmy przykładowe obrazy oraz ich rekonstrukcje:"
      ],
      "metadata": {
        "id": "LzFYi4kxGLMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "metadata": {
        "id": "251jlzNlybNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstructed\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-z7IRPBKyfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli nie jesteś zadowolona/y z jakości wyników, możesz pobawić się siecią/hiperparametrami użytymi podczas treningu. W tym przypadku umieść informację o zastosowanych zmianach w raporcie."
      ],
      "metadata": {
        "id": "62HgWJuhIT71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zaimplementuj funkcję do wizualizacji reprezentacji obrazów ze zbioru testowego w ukrytej przestrzeni 2-wymiarowej. Wyświetl wizualizację."
      ],
      "metadata": {
        "id": "ZygZn2TCGUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_space(model, data):\n",
        "  pass"
      ],
      "metadata": {
        "id": "Nn_rRP_2GfN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_space(autoencoder, x_test)"
      ],
      "metadata": {
        "id": "--F7_wwRyvp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz wyświetlmy obrazy powstałe przez dekodowanie wartości z prostokątnej siatki. Dobierz granice siatki, analizując wyniki funkcji plot_latent_space."
      ],
      "metadata": {
        "id": "chTvKghhGmmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_images(model, n, digit_size=28):\n",
        "  \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
        "\n",
        "  grid_x = np.linspace(..., ..., n)\n",
        "  grid_y = np.linspace(..., ..., n)\n",
        "  image_width = digit_size*n\n",
        "  image_height = image_width\n",
        "  image = np.zeros((image_height, image_width))\n",
        "\n",
        "  for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "      z = np.array([[xi, yi]])\n",
        "      x_decoded = model.decoder(z)\n",
        "      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
        "      image[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(image, cmap='Greys_r')\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oVUUmT37zUcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_images(autoencoder, 20)"
      ],
      "metadata": {
        "id": "-kvURdnmzoCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 1.3. Wybierz ze zbioru testowego dwa obrazy z różnymi liczbami. Dobierz takie liczby, dla których spodziewasz się, że odkodowanie średniej z ich zenkodowanych reprezentacji będzie miało sens. Wybierz dwie takie pary.  \n",
        "Dla każdej z par:  \n",
        "* Wyświetl wybrane liczby.  \n",
        "* Użyj enkodera do uzyskania 2-wymiarowych reprezentacji każdej liczby.  \n",
        "* Wylicz średnią z tych reprezentacji.  \n",
        "* Użyj dekodera na uzyskanej średniej.  \n",
        "* Wyświetl wynik.\n",
        "* Skomentuj wynik - czy przypomina jakąś liczbę? Czy takiego wyniku się spodziewałaś/eś?  \n",
        "\n",
        "(0.25pkt)\n",
        "\n",
        "\n",
        "Łączna liczba punktów do uzyskania za tę część: 0.75"
      ],
      "metadata": {
        "id": "CRu-Bqv0IL4E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWpF7nt0os8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. AutoEnkoder wariacyjny (VAE)"
      ],
      "metadata": {
        "id": "r5NyJWEwmgjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prosty AE nadaje się do różnych zadań, jak np. usuwanie szumu czy znajdowanie anomalii, ale nie jest najlepszą opcją do generowania obrazów z danego rozkładu (co widzieliśmy powyżej). W tej części zajmiemy się modelem znacznie lepiej nadającym się do tego zadania - autoenkoderem wariacyjnym.  \n",
        "  \n",
        "Zacznijmy od preprocessingu danych. Zwróć uwagę na reprezentację obrazów - tym razem będzie ona binarna - piksele będą mieć wartości 0/1. Będzie to miało wpływ na wybór funkcji kosztu! Moglibyśmy, oczywiście, dalej reprezentować obrazy jako wartości z całego zakresu [0; 1] i wtedy wykorzystać ten sam co wcześniej błąd rekonstrukcji, ale ponieważ takie podejście sprawdziliśmy wcześniej, tym razem zróbmy to inaczej."
      ],
      "metadata": {
        "id": "m_I32d51KFTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images_vae(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "x_train = preprocess_images_vae(x_train_raw)\n",
        "x_test = preprocess_images_vae(x_test_raw)"
      ],
      "metadata": {
        "id": "sJBj9dxuXYG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_raw, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "rPlbxy8NXhMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = x_train.shape[0]\n",
        "test_size = x_test.shape[0]\n",
        "val_size = x_val.shape[0]\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "P98TV2BpXk06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
        "                 .shuffle(train_size).batch(batch_size))\n",
        "val_dataset = (tf.data.Dataset.from_tensor_slices(x_val)\n",
        "               .shuffle(val_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "metadata": {
        "id": "iisfTt4cXo8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniujmy model.  \n",
        "Na co szczególnie warto zwrócić uwagę?  \n",
        "* Enkoder: ostatnia warstwa to warstwa gęsta, z liczbą neuronów równą 2 * rozmiar przestrzeni ukrytej. Te neurony będą kodować wartości średniej oraz wariancji rozkładu normalnego, z którego próbki chcemy podawać na wejście dekodera podczas generacji obrazów. W praktyce zamiast wariancji neurony kodują wartości _logvar_ - czyli logarytmu z wariancji. Dzięki temu, że logarytm może przyjmować dowolne rzeczywiste wartości, unikamy konieczności optymalizacji wartości niezerowych.  \n",
        "* Dekoder: zwróć uwagę na brak aktywacji w ostatniej warstwie dekodera, pomimo, że przecież generujemy obrazy z wartościami pikseli $\\in$ {0, 1}. Jest to związane z preferowanym sposobem wyliczania funkcji kosztu, którą w tym wypadku jest cross-entropia. Zwróć uwagę na wywołanie funkcji _sigmoid_cross_entropy_with_logits_ z poziomu _compute_loss_. Moglibyśmy równie dobrze użyć aktywacji sigmoid, a następnie wyliczyć cross-entropię, ale podejście wykorzystujące _sigmoid_cross_entropy_with_logits_ jest bardziej stabilne numerycznie."
      ],
      "metadata": {
        "id": "ibTmC-dSLzfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            # No activation\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "metadata": {
        "id": "iEokCEMPmWW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 2.1. Dlaczego powyższa implementacje CVAE nie stosuje żadnej aktywacji w ostatniej warstwie enkodera? Czy jakaś funkcja by się tutaj nadawała? (0.25pkt)"
      ],
      "metadata": {
        "id": "a_9MAfUrPsZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniujmy dodatkowe funkcje: wyliczanie kosztu, wykonanie kroku treningu.  \n",
        "Zwróć uwagę na funkcję kosztu. Składa się ona z trzech członów:  \n",
        "$log P(X|z)$ - człon związany z odkodowanymi obrazami. Można go interpretować jako błąd rekonstrukcji.  \n",
        "$logP(z)$ - człon związany z rozkładem, z którego losujemy elementy na wejście dekodera (prior).  \n",
        "$logQ(z|X)$ - człon związany z aproksymowanym rozkładem $P(z|X)$ z pomocą prostszego rozkładu $Q(z|X)$ (posterior)."
      ],
      "metadata": {
        "id": "KDXsRHmsQMOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "  This function computes the loss and gradients, and uses the latter to\n",
        "  update the model's parameters.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "jZ4EDxxgmh34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zdefiniujmy parametry treningu."
      ],
      "metadata": {
        "id": "FAqaoKtoQW-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "latent_dim = 2\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "F4ovR_s6reo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po każdej epoce będziemy wizualizować generację obrazów powstałych przez podanie na wejściu dekodera reprezentacji powstałej z tych samych obrazów ze zbioru testowego. W ten sposób będziemy mogli obserwować poprawę jakości generacji w każdej epoce."
      ],
      "metadata": {
        "id": "57rPbgw4Qfzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_sample):\n",
        "  mean, logvar = model.encode(test_sample)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  predictions = model.sample(z)\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "IcISsMCerxDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples_to_generate = 16\n",
        "\n",
        "# Pick a sample of the test set for generating output images\n",
        "assert batch_size >= num_examples_to_generate\n",
        "for test_batch in test_dataset.take(1):\n",
        "  test_sample = test_batch[0:num_examples_to_generate, :, :, :]"
      ],
      "metadata": {
        "id": "2o4GQCeJr3J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zainicjalizujmy model i przeprowadźmy trening:"
      ],
      "metadata": {
        "id": "DxsfwJTSRZsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CVAE(latent_dim)"
      ],
      "metadata": {
        "id": "wlU7ehlsQ6yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_save_images(model, 0, test_sample)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x in train_dataset:\n",
        "    train_step(model, train_x, optimizer)\n",
        "  end_time = time.time()\n",
        "\n",
        "  loss = tf.keras.metrics.Mean()\n",
        "  for val_x in val_dataset:\n",
        "    loss(compute_loss(model, val_x))\n",
        "  elbo = -loss.result()\n",
        "  clear_output()\n",
        "  print('Epoch: {}, Val set ELBO: {}, time elapse for current epoch: {}'\n",
        "        .format(epoch, elbo, end_time - start_time))\n",
        "  generate_and_save_images(model, epoch, test_sample)"
      ],
      "metadata": {
        "id": "vthuyYzGr6Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z zapisanych podczas treningu obrazów możemy przygotować i wyświetlić gif:"
      ],
      "metadata": {
        "id": "MV3VoRJ1RyBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'cvae.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "metadata": {
        "id": "6LWvm4yNuBRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed.embed_file(anim_file)"
      ],
      "metadata": {
        "id": "5az5Vs-MuSNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narysujmy teraz, podobnie jak wcześniej dla AE, dwuwymiarową przestrzeń ukrytą. Zaimplementuj funkcję plot_latent_space, która zenkoduje zbiór danych, a następnie wyświetli każdy punkt wraz z odchyleniem standardowym."
      ],
      "metadata": {
        "id": "OSuhPPQJSmq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_space(model, data):\n",
        "  pass"
      ],
      "metadata": {
        "id": "rheWEteqTU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_space(model, x_test)"
      ],
      "metadata": {
        "id": "IWf0IfLevsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narysujmy również odkodowane obrazy, podając na wejście dekodera wartości z rozkładu normalnego:"
      ],
      "metadata": {
        "id": "UbToVeZhTlEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_latent_images(model, n, digit_size=28):\n",
        "  \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
        "\n",
        "  norm = tfp.distributions.Normal(0, 1)\n",
        "  grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "  grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "  image_width = digit_size*n\n",
        "  image_height = image_width\n",
        "  image = np.zeros((image_height, image_width))\n",
        "\n",
        "  for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "      z = np.array([[xi, yi]])\n",
        "      x_decoded = model.sample(z)\n",
        "      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
        "      image[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(image, cmap='Greys_r')\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gee318PhucnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_images(model, 20)"
      ],
      "metadata": {
        "id": "DEKh1LmCukT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 2.2. Skomentuj wynik uzyskany przy użyciu funkcji plot_latent_images. Zwróć uwagę na jakość/sensowność rysowanych liczb. Porównaj wykres do analogicznego wykresu dla modelu AE. Zamieść w raporcie wykresy. (0.25pkt)  \n",
        "  \n",
        "Zadanie 2.3. Porównaj wyniki funkcji _plot_latent_space_ dla AE oraz VAE. Zwróć uwagę na \"gęstość\" punktów oraz zakres wartości. Zamieść w raporcie wykresy. (0.25pkt)   \n",
        "  \n",
        "Zadanie 2.4. Dla tych samych par obrazów, na których pracowałaś/eś w ostatnim zadaniu dot. AE, przygotuj reprezentacje ukryte z pomocą wytrenowanego VAE i odkoduj średnie z reprezentacji. Skomentuj wyniki, porównaj z wynikami z AE. (0.25pkt)  \n",
        "  \n",
        "Zadanie 2.5. Wróć do funkcji _compute_loss_. Człony _logpz_ oraz _logqz\\_x_ związane są z obliczaniem KL-divergence pomiędzy $Q(z|X)$ oraz $P(z)$. Zakładamy, że oba te rozkłady są gaussowskie, stąd możemy wykorzystać wzór na KL-divergence dla dwóch rozkładów gaussowskich. Znajdź ten wzór oraz przepisz funkcję _compute_loss_ z jego wykorzystaniem. Zamieść w raporcie przygotowaną formułę. Wytrenuj model ponownie, porównaj wyniki z poprzednią implementacją _compute_loss_. (0.25pkt)\n",
        "\n",
        "\n",
        "UWAGA: jeśli na którymkolwiek etapie nie jesteś zadowolona/y z działania modelu, możesz dokonać jego modyfikacji (podobnie jak dla AE).\n",
        "\n",
        "Łączna liczba punktów do uzyskania za tę część: 1.25"
      ],
      "metadata": {
        "id": "mC7BZi_eT_x-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LkwHCdso_8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Conditioned VAE"
      ],
      "metadata": {
        "id": "LbQPwr2BfHBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tej części rozszerzymy model VAE poprzez dodanie do modelu informacji o generowanym obrazie. Przygotujemy nowy zbiór danych, bazujący na MNIST, który będzie zawierał cyfry w różnych miejscach. Będziemy rozważać 9 pozycji, rozmieszczonych na siatce 3x3. Dodatkową informacją będzie 12-elementowy wektor, zawierający: etykietę (one-hot), położenie x, położenie y cyfry.  \n",
        "\n",
        "Zacznijmy od przygotowania zbioru danych. Będziemy generować obrazy w rozmiarze (42, 42), a bazowe cyfry będą mieć rozmiar (14, 14)."
      ],
      "metadata": {
        "id": "0XTAY0gZjmou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_small = tf.image.resize(x_train, (14, 14)).numpy()\n",
        "x_val_small = tf.image.resize(x_val, (14, 14)).numpy()\n",
        "x_test_small = tf.image.resize(x_test, (14, 14)).numpy()"
      ],
      "metadata": {
        "id": "sVc6jCKGiyCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()"
      ],
      "metadata": {
        "id": "S4mYaMiClgC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = encoder.fit_transform(y_train.reshape((-1, 1))).toarray()\n",
        "y_val = encoder.transform(y_val.reshape((-1, 1))).toarray()\n",
        "y_test = encoder.transform(y_test.reshape((-1, 1))).toarray()"
      ],
      "metadata": {
        "id": "Cu7iTT1glgj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_small.shape, y_train.shape"
      ],
      "metadata": {
        "id": "80FfRs7fj-RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij funkcję, która z każdego obrazu ze zbioru danych x tworzy 9 obrazów z cyfrą na każdej z 9 pozycji siatki 3x3, a także tworzy etykiety y w postaci wektora [cyfra-one-hot, pozycja_x, pozycja_y]. Dla każdej pary z oryginalnego zbioru danych (obraz, etykieta) wylosuj num_imgs par, które znajdą się w docelowym zbiorze danych (nie zapisujemy wszystkich 9 możliwości ze względu na ograniczenia RAM)."
      ],
      "metadata": {
        "id": "svnnwB8zr-Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conditioned_mnist(x, y, num_imgs=2):\n",
        "  x_res = np.empty(shape=(..., ..., ...), dtype='float32')  # pusta macierz z wynikami - obrazy x\n",
        "  y_res = np.empty(shape=(..., ...), dtype='float32')  # pusta macierz z wynikami - wektor y: etykieta (10 liczb), pozycja x, pozycja y\n",
        "  empty_res = np.zeros(shape=(..., ...))  # obraz wynikowy w docelowym rozmiarze, wypełniony zerami\n",
        "\n",
        "  for el, (arr, label) in enumerate(zip(x, y)):\n",
        "    to_sample_x = np.empty((9, x.shape[1]*3, x.shape[2]*3), dtype='float32')  # macierz przechowująca 9 wersji obrazu\n",
        "    to_sample_y = np.empty((9, 12), dtype='float32')  # macierz przechowująca 9 wersji etykiet\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        curr_x = empty_res.copy()\n",
        "        curr_x[i*x.shape[1]: (i+1)*x.shape[1], j*x.shape[2]: (j+1)*x.shape[2]] = arr.reshape((x.shape[1], x.shape[2]))\n",
        "        curr_y = [*label, i/2, j/2]  # normalizacja\n",
        "        to_sample_x[3*i+j] = curr_x\n",
        "        to_sample_y[3*i+j] = curr_y\n",
        "    idxs = ...  # wylosuj num_imgs indeksów z zakresu [0; 8] jako wektor numpy\n",
        "    x_res[el*num_imgs: (el+1)*num_imgs] = ...\n",
        "    y_res[el*num_imgs: (el+1)*num_imgs] = ...\n",
        "  x_res = x_res.reshape((-1, x.shape[1]*3, x.shape[2]*3, 1))\n",
        "  return x_res, y_res\n"
      ],
      "metadata": {
        "id": "EEP5Ru0xs0kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotujmy zbiór danych, zawierający po 2 wersje dla każdego obrazu:"
      ],
      "metadata": {
        "id": "aGzNCYDOuaX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_imgs = 2"
      ],
      "metadata": {
        "id": "KJxPg7K7qlzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cond, y_train_cond = conditioned_mnist(x_train_small, y_train, num_imgs)"
      ],
      "metadata": {
        "id": "s2iOGDKhfg7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cond.shape, y_train_cond.shape"
      ],
      "metadata": {
        "id": "hupG__4Eihvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_cond, y_val_cond = conditioned_mnist(x_val_small, y_val)\n",
        "x_test_cond, y_test_cond = conditioned_mnist(x_test_small, y_test)"
      ],
      "metadata": {
        "id": "PsQVK9tBmDuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij klasę Cond_CVAE na podstawie klasy CVAE. W tym celu:  \n",
        "1. Uzupełnij funkcję prepare_encoder. Będziemy mieć dwa wejścia do modelu - jedno na obraz, jedno na wektor cech [etykieta, pos_x, pos_y]. Przeprocesuj obraz z pomocą warstw konwolucyjnych (możesz wykorzystać implementację z CVAE). Możesz też przygotować kilka warstw, które zajmą się wektorem cech. Użyj warstwy konkatenującej wyniki z przetwarzania obrazu i wektora cech. Za tą warstwą znajdzie się warstwa gęsta, wyliczająca średnią i logvar.  \n",
        "  \n",
        "2. Uzupełnij funkcję prepare_decoder. Tu również mamy do czynienia z dwoma wejściami - jedno przyjmuje szum, drugie wektor cech. Połącz oba wejścia i przygotuj dekoder. Możesz skorzystać z implementacji CVAE, ale będą potrzebne zmiany związane z innym rozmiarem obrazów.\n",
        "\n",
        "Pozostałe funkcje są już zaimplementowane. Przyjrzyj się im. Co się zmieniło względem implementacji CVAE?\n"
      ],
      "metadata": {
        "id": "aWqEnHrwuy3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cond_CVAE(tf.keras.Model):\n",
        "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Cond_CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = self.prepare_encoder()\n",
        "    self.decoder = self.prepare_decoder()\n",
        "\n",
        "  def prepare_encoder(self):\n",
        "    input_img = tf.keras.layers.Input(shape=(42, 42, 1))\n",
        "    input_cond = tf.keras.layers.Input(shape=(12, ))\n",
        "    ...\n",
        "    x = tf.keras.layers.Concatenate()([..., ...])\n",
        "    # No activation\n",
        "    x = tf.keras.layers.Dense(latent_dim + latent_dim)(x)\n",
        "    return  tf.keras.Model([input_img, input_cond], [x])\n",
        "\n",
        "  def prepare_decoder(self):\n",
        "    input_latent = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "    input_cond = tf.keras.layers.Input(shape=(12, ))\n",
        "    inputs = tf.keras.layers.Concatenate()([input_latent, input_cond])\n",
        "    ...\n",
        "    return  tf.keras.Model([input_latent, input_cond], [x])\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, cond, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode([eps, cond], apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, inputs, apply_sigmoid=False):\n",
        "    logits = self.decoder(inputs)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "metadata": {
        "id": "gheKKCVrvUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotowanie do treningu:"
      ],
      "metadata": {
        "id": "7PegSh30xuJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "S78p9MZgo99Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples_to_generate = 16"
      ],
      "metadata": {
        "id": "K1HLoR4rx-li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nie będziemy tym razem wizualizować przestrzeni ukrytej, ale zacznijmy od latent_dim=2."
      ],
      "metadata": {
        "id": "CWJfdk-8x_Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2\n",
        "model = Cond_CVAE(latent_dim)"
      ],
      "metadata": {
        "id": "_j1Evumlx8Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train_cond).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(x_val_cond).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(x_test_cond).batch(batch_size)\n",
        "\n",
        "train_dataset_cond = tf.data.Dataset.from_tensor_slices(y_train_cond).batch(batch_size)\n",
        "val_dataset_cond = tf.data.Dataset.from_tensor_slices(y_val_cond).batch(batch_size)\n",
        "test_dataset_cond = tf.data.Dataset.from_tensor_slices(y_test_cond).batch(batch_size)\n",
        "\n",
        "train_dataset_with_cond = tf.data.Dataset.zip((train_dataset, train_dataset_cond)).shuffle(train_size*num_imgs)\n",
        "val_dataset_with_cond = tf.data.Dataset.zip((val_dataset, val_dataset_cond)).shuffle(val_size*num_imgs)\n",
        "test_dataset_with_cond = tf.data.Dataset.zip((test_dataset, test_dataset_cond)).shuffle(test_size*num_imgs)"
      ],
      "metadata": {
        "id": "nOELJ0WNryKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a sample of the test set for generating output images\n",
        "assert batch_size >= num_examples_to_generate\n",
        "for test_batch in test_dataset_with_cond.take(1):\n",
        "  test_sample_data = test_batch[0][0:num_examples_to_generate, :, :, :]\n",
        "  test_sample_cond = test_batch[1][0:num_examples_to_generate, :]\n",
        "test_sample = [test_sample_data, test_sample_cond]"
      ],
      "metadata": {
        "id": "fQavvy7Vz1L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij funkcję kosztu:"
      ],
      "metadata": {
        "id": "6trhoyEfyqeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, x):\n",
        "  data, cond = x\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = ...\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=data)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
      ],
      "metadata": {
        "id": "YLLts7NOtU38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ostatnie definicje/inicjalizacje:"
      ],
      "metadata": {
        "id": "cb3T38QpzAf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_sample):\n",
        "  data, cond = test_sample\n",
        "  mean, logvar = model.encode(test_sample)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  predictions = model.sample(cond, z)\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "WxwLi2Kg7D-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "  This function computes the loss and gradients, and uses the latter to\n",
        "  update the model's parameters.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "metadata": {
        "id": "IDvwq5n7170v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "eiGZim882Az0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...i zaczynamy trening."
      ],
      "metadata": {
        "id": "63zlg8zRzHBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_save_images(model, 0, test_sample)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x in train_dataset_with_cond:\n",
        "    train_step(model, train_x, optimizer)\n",
        "  end_time = time.time()\n",
        "\n",
        "  loss = tf.keras.metrics.Mean()\n",
        "  for val_x in val_dataset_with_cond:\n",
        "    loss(compute_loss(model, val_x))\n",
        "  elbo = -loss.result()\n",
        "  clear_output()\n",
        "  print('Epoch: {}, Val set ELBO: {}, time elapse for current epoch: {}'\n",
        "        .format(epoch, elbo, end_time - start_time))\n",
        "  generate_and_save_images(model, epoch, test_sample)"
      ],
      "metadata": {
        "id": "ql5UNumjpGH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'cond_cvae.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "metadata": {
        "id": "niMpvCNmGU1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed.embed_file(anim_file)"
      ],
      "metadata": {
        "id": "nV1OuiurGXQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli model zupełnie nie radzi sobie z zadaniem, pobaw się definicją modelu bądź hiperparametrami (głównie batch_size, liczba epok). Model powinien zwracać rozsądne wyniki zarówno w konktekście położenia, jak i rodzaju generowanej cyfry."
      ],
      "metadata": {
        "id": "JBsrVl-71DSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 3.1. Sprawdź jakość modelu dla 3 różnych wartości latent_dim (trzeba dla każdej z nich osobno wytrenować model). Niech będą od siebie znacząco różne, np. 2, 25, 100. Przy większym latent_dim może być potrzebnych więcej epok.  \n",
        "Punktacja:  \n",
        "Skuteczny trening dla jednej wartości latent dim = 1 pkt.  \n",
        "0.5 pkt za testy dla dwóch innych wartości latent_dim (po 0.25 za każdą). Punkty będą przyznane, jeśli model będzie w stanie generować _sensowne_ wyniki.  \n",
        "  \n",
        "Zadanie 3.2. Wykonaj dla najlepszego modelu z punktu 3.1.:\n",
        "* Wybierz przykład ze zbioru testowego (obraz + etykieta).  \n",
        "* Przepuść próbkę przez enkoder, uzyskaj reprezentację _z_.  \n",
        "* Dla każdego z 9 możliwych wektorów [poprawna_etykieta, pos_x, pos_y] przepuść przez dekoder reprezentację _z_ wraz z informacją o etykiecie i położeniu. Wyświetl uzyskany obraz. Skomentuj wyniki - czy za każdym razem uzyskano oczekiwaną liczbę w oczekiwanym miejscu? Jeśli nie, to co może być przyczyną? (0.25 pkt)  \n",
        "\n",
        "Zadanie 3.3. Powtórz zadanie 3.2, ale tym razem jako reprezentację _z_ wykorzystaj wartości wylosowane z rozkładu normalnego oraz wybierz dowolną etykietę. Skomentuj wyniki - czy za każdym razem uzyskano oczekiwaną liczbę w oczekiwanym miejscu? (0.25 pkt)\n",
        "\n",
        "\n",
        "Za tę część: 2 punkty"
      ],
      "metadata": {
        "id": "-UEFGkORzVP9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "29bit2db8KPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Conditioned GAN"
      ],
      "metadata": {
        "id": "nYc_v5yOdThd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W ostatniej części wytrenujemy sieć GAN na obrazach ze zbioru MNIST. Ponownie przygotujemy implementację z dodatkowym wektorem wejściowym - tym razem będzie to wektor jednoelementowy z etykietą."
      ],
      "metadata": {
        "id": "GzAM3HzaPsxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wracamy do ciągłej reprezentacji obrazów:"
      ],
      "metadata": {
        "id": "aOzMuZBO4_Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images_gan(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return images.astype('float32')\n",
        "\n",
        "x_train = preprocess_images_gan(x_train_raw)\n",
        "y_train = encoder.transform(y_train_raw.reshape((-1, 1))).toarray()"
      ],
      "metadata": {
        "id": "zTbyOhUKdVby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "OFxVuzx8o1Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = x_train.shape[0]\n",
        "\n",
        "batch_size = 256"
      ],
      "metadata": {
        "id": "_z3_WNgmeEmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przygotujmy dataset. Będzie on zawierał:\n",
        "* Obrazy.\n",
        "* Dodatkowe wektory wejściowe z etykietami.\n",
        "* Wektory wejściowe z etykietami do generowania przykładów przez generator w trakcie treningu."
      ],
      "metadata": {
        "id": "DOuguIZVQHkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)\n",
        "train_dataset_cond = tf.data.Dataset.from_tensor_slices(y_train).batch(batch_size)\n",
        "train_dataset_fake_cond = tf.data.Dataset.from_tensor_slices(y_train).shuffle(train_size).batch(batch_size)\n",
        "\n",
        "train_dataset_with_cond = tf.data.Dataset.zip((train_dataset, train_dataset_cond, train_dataset_fake_cond)).shuffle(train_size)"
      ],
      "metadata": {
        "id": "AJ1OacuJKsLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij definicję generatora:"
      ],
      "metadata": {
        "id": "8K67UUpnRTGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_generator(latent_dim, cond_dim):\n",
        "  input_img = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  input_cond = ...\n",
        "  inputs = tf.keras.layers.Concatenate(axis=1)([...])\n",
        "\n",
        "  x1 = tf.keras.layers.Dense(7*7*256, use_bias=False)(inputs)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "  x1 = tf.keras.layers.Reshape((7, 7, 256))(x1)\n",
        "\n",
        "  x1 = tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x1)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "\n",
        "  x1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x1)\n",
        "  x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "  x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "\n",
        "  x1 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation=...)(x1)\n",
        "\n",
        "  return tf.keras.Model([input_img, input_cond], x1, name='generator')"
      ],
      "metadata": {
        "id": "K4lKSyAw8G-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "cond_dim = 10"
      ],
      "metadata": {
        "id": "VlfrHn8uHBYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wyświetlmy obraz wygenerowany przez niewytrenowany generator przy podaniu na wejściu etykiety \"1\":"
      ],
      "metadata": {
        "id": "ETeUZA-zRmth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = prepare_generator(latent_dim, cond_dim)\n",
        "\n",
        "noise = tf.random.normal([1, latent_dim])\n",
        "label = tf.constant(np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]).reshape(1, -1))\n",
        "generated_image = generator([noise, label], training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "id": "WigdLrhReZgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij definicję dyskryminatora:"
      ],
      "metadata": {
        "id": "NyZ0t-ZGRwYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_discriminator(img_shape, cond_dim):\n",
        "  input_img = tf.keras.layers.Input(shape=img_shape)\n",
        "  x = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(input_img)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "  input_cond = tf.keras.layers.Input(shape=(cond_dim,))\n",
        "  x = tf.keras.layers.Concatenate(axis=1)([x, input_cond])\n",
        "  ...  # dodaj kilka warstw\n",
        "\n",
        "  x = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "  return tf.keras.Model([input_img, input_cond], x, name='discriminator')"
      ],
      "metadata": {
        "id": "Cy8lRCp2ehSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = (28, 28, 1)"
      ],
      "metadata": {
        "id": "xiMUyPRbIv2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zobaczmy predykcję niewytrenowanego dyskryminatora:"
      ],
      "metadata": {
        "id": "dB5scvniSD9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = prepare_discriminator(img_shape, cond_dim)\n",
        "decision = discriminator([generated_image, label])\n",
        "print(decision)"
      ],
      "metadata": {
        "id": "Uf_8SkegelTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "4AXJVr7Kergd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij funkcje kosztu:"
      ],
      "metadata": {
        "id": "xR2-_EO0SNC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    # real_output, fake_output - predykcje dyskryminatora\n",
        "    real_loss = cross_entropy(..., real_output)\n",
        "    fake_loss = cross_entropy(..., fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "qIAs4wjle8aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    # fake_output - predykcja dyskryminatora\n",
        "    return cross_entropy(..., fake_output)"
      ],
      "metadata": {
        "id": "lYJ80Y6pfDcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicjalizacje:"
      ],
      "metadata": {
        "id": "rT0X3am2S7wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "QSd1WvhGfI9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "seed_cond = ... # uzupełnij losowymi num_examples_to_generate etykietami one-hot"
      ],
      "metadata": {
        "id": "jG_hTuD0fV7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uzupełnij funkcję train_step:"
      ],
      "metadata": {
        "id": "T_a-OYgBTEK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(data):\n",
        "    images, cond, noise_cond = data\n",
        "    batch_size = tf.shape(images)[0]\n",
        "\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator([..., ...], training=True)\n",
        "\n",
        "      real_output = discriminator([..., ...], training=True)\n",
        "      fake_output = discriminator([..., ...], training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "metadata": {
        "id": "uKQtoSCVfY9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Czas na trening!"
      ],
      "metadata": {
        "id": "E383Xh06Ta75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "nI6xYaYafgXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    clear_output()\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             [seed, seed_cond])\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  clear_output()\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           [seed, seed_cond])"
      ],
      "metadata": {
        "id": "EWVt0ROVfdla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset_with_cond, EPOCHS)"
      ],
      "metadata": {
        "id": "rOycVlOGfi3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'gan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "metadata": {
        "id": "DI7QmjBuf5IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed.embed_file(anim_file)"
      ],
      "metadata": {
        "id": "Fnp4xlUjf7o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zadanie 4.1. Wygeneruj po jednym obrazie z każdą liczbą z pomocą generatora. Oceń jakość wyników. Jeśli jakość modelu pozostawia wiele do życzenia, spróbuj go poprawić, np. zwiększając liczbę epok bądź zmieniając definicję generatora/dyskryminatora.  \n",
        "Punktacja: 1 pkt za poprawnie przeprowadzony trening i wykonane zadanie 4.1."
      ],
      "metadata": {
        "id": "HCt-cziNTdhN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D1eKZE-kRCo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}