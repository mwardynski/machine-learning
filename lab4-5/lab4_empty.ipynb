{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377e2931-3fd4-48eb-b7c0-84fbadbb3850",
   "metadata": {},
   "source": [
    "# Time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a24c2-8b01-4e51-81dc-8123b193e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bafa4-8d2f-4c99-910e-c9f0ea05ac8a",
   "metadata": {},
   "source": [
    "We will use [sktime](https://www.sktime.net/en/stable/index.html) as our main library for time series. It offers interface very similar to scikit-learn, and conveniently wraps many other libraries, for example:\n",
    "- [statsforecast](https://github.com/Nixtla/statsforecast) - efficient implementations of many forecasting methods, e.g. AutoARIMA and AutoETS\n",
    "- [pmdarima](https://alkaline-ml.com/pmdarima/) - statistical tests for time series and another AutoARIMA implementation\n",
    "- [statsmodels](https://www.statsmodels.org/stable/index.html) - a few time series decomposition and forecasting methods\n",
    "\n",
    "For statistical tests we will use [scipy](https://docs.scipy.org/doc/scipy/index.html) and [statsmodels](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00201d3-569e-478c-87d2-066d76546496",
   "metadata": {},
   "source": [
    "## Forecasting Polish inflation\n",
    "\n",
    "The problem of forecasting inflation (here defined using consumer price index, CPI) is very common, done by basically every country and larger financial institutions. In practice it's not a single task, but rather a collection of related problems, forecasting e.g. inflation, core inflation (excluding most volatile components, e.g. food and energy prices), and other formulations.\n",
    "\n",
    "In Poland, basic data about inflation [is published by the Central Statistical Office of Poland (GUS)](https://stat.gov.pl/obszary-tematyczne/ceny-handel/wskazniki-cen/wskazniki-cen-towarow-i-uslug-konsumpcyjnych-pot-inflacja-/miesieczne-wskazniki-cen-towarow-i-uslug-konsumpcyjnych-od-1982-roku/), with monthly, quarterly, half-yearly and yearly frequency. More detailed information is published by other institutions, because they depend on the methodology used, e.g. core inflation [is calculated and published by the National Bank of Poland (NBP)](https://nbp.pl/statystyka-i-sprawozdawczosc/inflacja-bazowa/).\n",
    "\n",
    "Forecasting inflation is a challenge, since it typically:\n",
    "- has visible cycles, but very irregular\n",
    "- is implicitly tied to many external factors (global economy, political decisions etc.)\n",
    "- there is no apparent seasonality\n",
    "- we are interested in forecasting with many frequencies, e.g. monthly (short-term decisions) and yearly (long-term decisions)\n",
    "\n",
    "We will use GUS data with monthly frequency. To get a percentage value (annual percentage rate inflation) from the raw data, we need to subtract 100 from provided values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffa2d6-a1ec-4c91-8998-a104f91597b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"polish_inflation.csv\")\n",
    "df = df.rename(columns={\"Rok\": \"year\", \"Miesiąc\": \"month\", \"Wartość\": \"value\"})\n",
    "\n",
    "# create proper date column\n",
    "df[\"day\"] = 1\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "df[\"date\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "# set datetime index\n",
    "df = df.set_index(df[\"date\"], drop=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "# leave only time series values\n",
    "df = df[\"value\"] - 100\n",
    "\n",
    "# filter out NaN values from the end of the series\n",
    "df = df[~df.isna()]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edfd8d-7ed8-4a74-8771-b9c6860a5a85",
   "metadata": {},
   "source": [
    "To plot the time series, the easiest way is to use the [plot_series() function](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.utils.plotting.plot_series.html) from sktime, which will automatically nicely format X and Y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d766f-c747-440d-80af-ee5a430514ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "plot_series(df, title=\"Polish inflation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125ed4-9c94-4823-a3f1-5474cfd290ca",
   "metadata": {},
   "source": [
    "There is no error here - 90s were a particularly interesting period, with [hyperinflation](https://pl.wikipedia.org/wiki/Hiperinflacja#Polska_%E2%80%93_lata_80._XX_wieku), later [\"shock therapy\"](https://en.wikipedia.org/wiki/Shock_therapy_(economics)) and implementation of the [Balcerowicz Plan](https://pl.wikipedia.org/wiki/Plan_Balcerowicza). From the perspective of time series forecasting, this is definitely na outlier, but quite long. For this reason, we will limit ourselves to post-2000 data.\n",
    "\n",
    "Similar behavior can often be seen in time series data, related to e.g. [2007-2008 financial crisis](https://en.wikipedia.org/wiki/2007%E2%80%932008_financial_crisis) or COVID-19 pandemic. Such events can introduce shocks with long effects, and using only later data is arguably the simplest strategy to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8e794-7430-412c-843c-9c5bdb5fa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index >= \"2000-01\"]\n",
    "plot_series(df, title=\"Polish inflation, from year 2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e593b4-9e1a-4ce9-916c-caf179b36b07",
   "metadata": {},
   "source": [
    "There is definitely some information here, with cycles and trends. Fortunately, the data seems to be changing reasonably slowly most of the time. But what about seasonality?\n",
    "\n",
    "**Exercise 1 (0.5 points)**\n",
    "\n",
    "Implement the `plot_stl_decomposition` function. Use `STLTransformer` to compute the STL decomposition ([documentation](https://www.sktime.net/en/v0.29.0/api_reference/auto_generated/sktime.transformations.series.detrend.STLTransformer.html)). Remember to use appropriate arguments to set the seasonality period and return all three components.\n",
    "\n",
    "Plot the resulting STL decomposition. Comment:\n",
    "- do you see a yearly seasonality here?\n",
    "- concerning residuals, are they only a white noise, or do they seem to contain some further information to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ff53a-a5bd-4fb3-a02b-e7ff4cd475a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.detrend import STLTransformer\n",
    "\n",
    "\n",
    "def plot_stl_decomposition(data: pd.Series, seasonal_period: int = 12) -> None:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a010c-b9f3-4e2c-ac9f-8b5663a32d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stl_decomposition(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ff1c8-fd63-4e5a-90f1-5aa7187748e2",
   "metadata": {},
   "source": [
    "// comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212cf20-957e-4ab7-a3b4-ed7f98babed1",
   "metadata": {},
   "source": [
    "Manual check using STL decomposition is useful - this allows us to gain intuition and knowledge about the data, and validation parameters. Of course we also have automated procedures, using statistical tests, to avoid such manual labor when we can.\n",
    "\n",
    "Let's check the seasonality and stationarity of our data. This is not strictly necessary for ETS models - they use the data as-is. However, the ARIMA models require stationary data, and knowledge about seasonality, or lack thereof, can greatly accelerate our experiments. SARIMA takes much longer than simpler ARIMA.\n",
    "\n",
    "**Exercise 2 (0.75 points)**\n",
    "\n",
    "1. Check, using statistical tests for seasonality, if there is a quarterly, half-yearly, or yearly seasonality in the data. Use the `nsdiffs` function from pmdarima ([documentation](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.nsdiffs.html)). If you detect seasonality, remove it using the `Differencer` from sktime ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.transformations.series.difference.Differencer.html)) and plot the deasonalized series.\n",
    "\n",
    "2. Check, using statistical tests for stationarity, what differencing order stationarizes the data. Use the `ndiffs` function from pmdarima ([documentation](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.ndiffs.html)). If it's greater than zero, i.e. differencing is necessary, then stationarize the series using the `Differencer` class and plot the resulting time series.\n",
    "\n",
    "3. Comment, which ARIMA model would you use, based on those findings, and why: ARMA, ARIMA, or SARIMA.\n",
    "\n",
    "Use the default `D_max` and `d_max` values.\n",
    "\n",
    "**Warning:** create new variables for values after differencing, do not overwrite the `df` variable. It will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff6d6-f238-49de-bd37-19f3490e346b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac3c4e1-699f-42c3-aefd-6d1ecabd20d8",
   "metadata": {},
   "source": [
    "// comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76688e73-8a44-4f06-b888-d872f50e809a",
   "metadata": {},
   "source": [
    "We are now basically ready to train our forecasting models. We will use 20% of the newest data for testing, using the expanding window strategy, with step 1 (we get inflation reading each month). MAE and MASE will be used as quality metrics.\n",
    "\n",
    "We will also perform residuals analysis. Errors should be normally distributed (unbiased mdoel) and do not have autocorrelation (model utilizing all available information). For all statistical tests we assume the significance level $\\alpha = 0.05$.\n",
    "\n",
    "For testing normality, the Anderson-Darling test is less conservative than Shapiro-Wilk test, which is quite useful in practice. Errors are very rarely close to \"true\" normality in real world. The null hypothesis is that values come from the given distributions (by default the normal one), and alternative hypothesis that they come from other distribution.\n",
    "\n",
    "For testing error autocorrelation, the Ljung-Box test is used, which tests autocorrelation for various lags. For each lag, a separate test is performed. The null hypothesis is the lack of autocorrelation, and the alternative hypothesis is that there is an autocorrelation with a given lag.\n",
    "\n",
    "**Exercise 3 (1.5 points)**\n",
    "\n",
    "Implement the missing parts of the `evaluate_model` function:\n",
    "1. Create `ExpandingWindowSplitter` ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.split.ExpandingWindowSplitter.html)), which should start testing at 80% of data. The forecast window size is controlled via the `horizon` parameter.\n",
    "2. Create a list of metric objects, consisting of MAE and MASE ([ocumentation](https://www.sktime.net/en/latest/api_reference/performance_metrics.html)).\n",
    "3. Perform the model evaluation, using the `evaluate` function ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.model_evaluation.evaluate.html)). Pass `return_data=True`, in order to also return the computed forecasts. It returns a DataFrame with results.\n",
    "4. Calculate average metric values, using the resulting DataFrame. Print them rounded to 2 decimal places.\n",
    "5. Taking into consideration the `analyze_residuals` argument, perform the error analysis:\n",
    "   - calculate residuals $y - \\hat{y}$\n",
    "   - plot the residuals histogram\n",
    "   - perform the Anderson-Darling test ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html)) and print whether the distribution is normal or not\n",
    "   - perform the Ljung-Box test ([documentation](https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html)) and print the test results\n",
    "\n",
    "Test the function, using two baseline forecasting methods: average (mean) and last known value. Use the `NaiveForecaster` class ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html)), with 3 months forecasting horizon. Plot the forecasts, using the `plot_forecasts` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb3c0e-ab5e-4f2a-9a9c-daaff029023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    MeanAbsoluteScaledError,\n",
    "    MeanAbsoluteError,\n",
    "    MeanAbsolutePercentageError,\n",
    ")\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    data: pd.Series,\n",
    "    horizon: int = 1,\n",
    "    plot_forecasts: bool = False,\n",
    "    analyze_residuals: bool = False,\n",
    ") -> None:\n",
    "    cv = ...\n",
    "    metrics = ...\n",
    "    results = ...\n",
    "    mae = ...\n",
    "    mase = ...\n",
    "\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MASE: {mase:.2f}\")\n",
    "    \n",
    "    y_pred = pd.concat(results[\"y_pred\"].values)\n",
    "\n",
    "    if plot_forecasts:\n",
    "        y_true = data[y_pred.index]\n",
    "        plot_series(data, y_pred, labels=[\"y\", \"y_pred\"])\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "    \n",
    "    if analyze_residuals:\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7eaf1-e2c5-4ba0-81bb-fac1d46cce38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d02d2f2-4af9-4143-8404-78c0812498d0",
   "metadata": {},
   "source": [
    "Results from our first baselines look reasonable. Let's see how ETS and ARIMA will compare.\n",
    "\n",
    "**Exercise 4 (0.75 points)**\n",
    "\n",
    "1. Perform forecasting using the AutoETS algorithm in the damped trend variant, based on the `statsforecast` implementation ([documentation](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.statsforecast.StatsForecastAutoETS.html)). Plot forecasts and perform residuals analysis.\n",
    "2. Similarly, use AutoARIMA for forecasting ([documentation](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.statsforecast.StatsForecastAutoARIMA.html)). If you didn't detect seasonality earlier, pass appropriate option to ignore SARIMA variants.\n",
    "3. Comment on the results:\n",
    "   - did you manage to outperform the baselines?\n",
    "   - which of the models is better, and what may this mean?\n",
    "   - which model is correct, at least approximately, i.e. has normally distributed, non-autocorrelated errors?\n",
    "   - are the results of the best model, subjectively, good enough?\n",
    "\n",
    "As before, use 3 month forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4ce0b-14b9-4e17-8be9-5d4b95cefce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a0786b-5b08-4a46-b853-8b590b7a2346",
   "metadata": {},
   "source": [
    "// comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51b89b-1a61-4bc3-9e02-dc14b28ff452",
   "metadata": {},
   "source": [
    "3 month horizon is quite short, generally speaking. The question is, what about long-term forecasting, e.g. half-yearly or yearly? They are equally, or even more interesting and relevant, e.g. for national budget planning.\n",
    "\n",
    "**Exercise 5 (0.75 points)**\n",
    "\n",
    "Perform forecasting for 6-month and yearly horizons, using:\n",
    "- both baselines\n",
    "- ETS\n",
    "- ARIMA\n",
    "\n",
    "For the best model, plot the forecasts and perform residuals analysis.\n",
    "\n",
    "Comment:\n",
    "- are there differences between models, compared to the 3-month forecasting?\n",
    "- how does the quality of forecasts change for longer horizons?\n",
    "- in your opinion, are those models useful at all for long-term forecasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925cd49-984e-4a13-805d-451789c27389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94726e94-4efe-439a-9c8b-07901b2e8c6b",
   "metadata": {},
   "source": [
    "// comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e6b60-5c1d-405b-8ba2-f5bd7dc4783f",
   "metadata": {},
   "source": [
    "## Forecasting network traffic\n",
    "\n",
    "And now for something completely different. Network traffic forecasting is necessary for virtual machines (VMs) scaling, adding more servers to handle load in parallel. This is done more and more frequently by using ML models, based on time series forecasting, to scale more intelligently and avoid manually tweaking scaling rules. This is called predictive scaling, and is implemented by e.g. [AWS](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html), [GCP](https://cloud.google.com/compute/docs/autoscaler/predictive-autoscaling), and [Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-predictive). There are also solutions for Kubernetes, both [open source](https://predictive-horizontal-pod-autoscaler.readthedocs.io/en/latest/) and [proprietary](https://keda.sh/blog/2022-02-09-predictkube-scaler/). Time series forecasting allows lower latency and lower costs, automatically turning off machines when low demand is predicted.\n",
    "\n",
    "Wikipedia and Google hosted [Kaggle competition](https://www.kaggle.com/c/web-traffic-time-series-forecasting), where the goal was predicting the network traffic on particular Wikipedia pages. It's a really massive dataset, so we will operate on a simplified problem, where we have a total number of requests to the Wikipedia domain in millions.\n",
    "\n",
    "Typical characteristics of such tasks are:\n",
    "- short-term forecasting\n",
    "- high frequency\n",
    "- dynamically changing, noisy data (e.g. bot activity, web scraping)\n",
    "- frequent model retraining\n",
    "- high need for automatization, lack of manual model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aef42e-b807-4528-aa29-5b93d695be96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"wikipedia_traffic.parquet\")\n",
    "df = df.set_index(\"date\").to_period(freq=\"d\")\n",
    "plot_series(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb9066-ac63-4b38-908b-c2fd712ecd09",
   "metadata": {},
   "source": [
    "**Exercise 6 (1 point)**\n",
    "\n",
    "For 1-day horizon, train models and evaluate them (similarly to the previous dataset, with 20% test data):\n",
    "- two baselines\n",
    "- ETS with damped trend\n",
    "- ARIMA (without seasonality)\n",
    "- SARIMA\n",
    "\n",
    "Comment:\n",
    "- based on those results, is there a seasonality here?\n",
    "- did you manage to outperform the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f96ad7-0dfd-4a58-8355-3ed9cd888d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d489afe-bb56-4006-a38a-c14fc983fb61",
   "metadata": {},
   "source": [
    "But maybe we can do better? This data is highly volatile, with high variance, which is particularly bad for ARIMA models. Let's apply the variance-stabilizing transform then. We have only positive values here, so there are no numerical problems.\n",
    "\n",
    "Note that `Pipeline` from sktime is needed here ([documentation](https://www.sktime.net/en/stable/api_reference/pipeline.html)), which will automatically invert the transformation during prediction. Sometimes models are evaluated on the transformed data, but we are generally interested in the forecasting quality on the data in its raw form. The goal of transformations is to make the training easier for the model.\n",
    "\n",
    "**Exercise 7 (0.5 points)**\n",
    "\n",
    "Create a pipeline, consisting of a transform object and AutoARIMA model (without seasonality). Try out the following transformations ([documentation](https://www.sktime.net/en/stable/api_reference/transformations.html)):\n",
    "- log\n",
    "- sqrt\n",
    "- Box-Cox\n",
    "\n",
    "Comment, whether the result is better after the transformation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a150d56-dc44-444c-89ed-d7227bbcff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "375400a6-3e06-43c7-8130-92e42d0f1d5f",
   "metadata": {},
   "source": [
    "## Sales forecasting\n",
    "\n",
    "Arguably the most common application of time series forecasting is predicting sales, demand, costs etc., so all typical operational indicators of a company. Basically every company has to do this, therefore even basic software like Excel or PowerBI have built-in capabilities for time series forecasting.\n",
    "\n",
    "We will focus on a task definitely vital for the Italian economy, i.e. the pasta sales. Dataset has been gathered by the Italian scientists for [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0957417421005431?via%3Dihub). Data covers years 2014-2018, from 4 companies offering various pasta-based products. They also contain data about promotions for particular products. There are also missing values, which must be imputed.\n",
    "\n",
    "Typical characteristics of this type of data are:\n",
    "- positive trend, smaller or larger (changing in time)\n",
    "- strong seasonality, often more than one\n",
    "- highly sensitive to recurring events, e.g. weekends or holidays\n",
    "- large outliers, often related to events\n",
    "- relatively low frequency, daily or less frequent\n",
    "- often long forecasting horizons, e.g. monthly, quarterly, yearly\n",
    "- rich exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35fd51-5c4c-4bb3-8f6f-662260c40537",
   "metadata": {},
   "source": [
    "**Exercise 8 (1 point)**\n",
    "\n",
    "1. Read the data from `\"italian_pasta.csv\"` file\n",
    "2. Select columns from company B1 (they have `\"B1\"` in their name) and `\"DATE\"` column.\n",
    "3. Create the `value` column with total pasta sales, i.e. sum of columns with `\"QTY\"` in name.\n",
    "4. Create the `num_promos` column with total number of promotions, i.e. sum of columns with `\"PROMO\"` in name.\n",
    "5. Leave only columns `\"DATE\"`, `\"value\"` and `\"num_promos\"`.\n",
    "6. Create index with type `datetime`:\n",
    "   - change type of `\"DATE\"` colum to `datetime`\n",
    "   - set its frequency as daily, `\"d\"`\n",
    "   - set it as index\n",
    "7. Split the data into:\n",
    "   - `y` variable, `pd.Series` created from the `\"value\"` column, our main time series values\n",
    "   - `X` variable, `pd.Series` created from the `\"num_promos\"` column, exogenous variables\n",
    "8. Impute the missing values in exogenous variables with zeros, assuming that by default there are no promotions.\n",
    "9. Plot the `y` time series. Remember to set the appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d1b17-432a-473e-b600-2f0d8dcb3657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf3b554-1f5c-4a68-a4ee-df7dabee6deb",
   "metadata": {},
   "source": [
    "We are interested in long-term forecasting. We assume that our client, an italian pasta maker, has the historical data from years 2014-2017 and wants to forecast the sales for 2018. Such information is required e.g. to make contracts for long-term supply of raw materials and next year production plans. From ML perspective this hard, since there is only a single temporal train-test split with long horizon, instead of expanding window, but it's faster.\n",
    "\n",
    "We will use the `evaluate_pasta_sales_model` function for evaluation.\n",
    "\n",
    "**Exercise 9 (1 point)**\n",
    "\n",
    "Implement the missing parts of the evaluation function:\n",
    "1. Split `y` into training and testing set with time split. Test set starts at `2018-01-01`.\n",
    "2. If user passes `X`, split it in the same way.\n",
    "3. Impute the missing values in `y`, using `Imputer` from sktime ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.transformations.series.impute.Imputer.html)) with `ffill` strategy (copy last known value).\n",
    "4. Train the model (remember to pass `X`) and perform prediction.\n",
    "5. Evaluate it using MAE and MASE functions ([documentation](https://www.sktime.net/en/stable/api_reference/performance_metrics.html)). Print the results rounded to 2 decimal places.\n",
    "6. Copy the code for `analyze_residuals` from exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58f021-c0ba-4d67-a245-b528912b0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    mean_absolute_scaled_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "\n",
    "def evaluate_pasta_sales_model(\n",
    "    model,\n",
    "    df: pd.Series,\n",
    "    X: Optional[np.ndarray] = None,\n",
    "    plot_forecasts: bool = False,\n",
    "    analyze_residuals: bool = False,\n",
    ") -> None:\n",
    "    y_train = ...\n",
    "    y_test = ...\n",
    "\n",
    "    if X is not None:\n",
    "        ...\n",
    "    else:\n",
    "        X_train = None\n",
    "        X_test = None\n",
    "\n",
    "    # impute\n",
    "    ...\n",
    "\n",
    "    # train and predict\n",
    "    ...\n",
    "    y_pred = ...\n",
    "\n",
    "    mae = ...\n",
    "    mape = ...\n",
    "    mase = ...\n",
    "\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}\")\n",
    "    print(f\"MASE: {mase:.2f}\")\n",
    "\n",
    "    if plot_forecasts:\n",
    "        y_test = df[y_pred.index]\n",
    "        plot_series(y_test, y_pred, labels=[\"y\", \"y_pred\"])\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "    \n",
    "    if analyze_residuals:\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb901ea-0be6-4fb0-81af-1f95010eda49",
   "metadata": {},
   "source": [
    "**Exercise 10 (1.5 points)**\n",
    "\n",
    "Perform the forecasting using the following models:\n",
    "- two baselines\n",
    "- ETS with damped trend\n",
    "- ARIMA\n",
    "- SARIMA with 30-day seasonality\n",
    "- ARIMAX\n",
    "- SARIMAX with 30-day seasonality\n",
    "\n",
    "For the best model also try the log, sqrt and Box-Cox transformations.\n",
    "\n",
    "For the final model plot the forecasts and perform residuals analysis.\n",
    "\n",
    "Comment:\n",
    "- did you outperform the baseline?\n",
    "- does the final model use seasonality and/or exogenous variables (data about promotions)?\n",
    "- was it worth it to use the variance-stabilizing transformation?\n",
    "- comment on the general behavior of the model on the test set, based on the forecast plot\n",
    "- is the model unbiased (normally distributed residuals with zero mean), without autocorrelation, or can this be improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41265b1-07ea-45d5-822c-d61809cc2755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecfa98c8-ce60-4cd8-8547-5649fd7a8578",
   "metadata": {},
   "source": [
    "Exogenous variables can be expanded with feature engineering. For example, the behavior of clients is quite different during weekends and holidays. Typically sales rise quite sharply before and after days when stores are closed, and falls to exactly zero when they have to be closed.\n",
    "\n",
    "**Exercise 11 (0.75 points)**\n",
    "\n",
    "1. Create a list of variables for holidays using `HolidayFeatures` ([documentation](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.transformations.series.holiday.HolidayFeatures.html)):\n",
    "   - use `country_holidays` function from the holidays library\n",
    "   - remember that we are processing italian data, with country identifier `\"IT\"`\n",
    "   - include weekends as holidays\n",
    "   - create a single variable \"is there a holiday\" (`return_dummies` and `return_indicator` options)\n",
    "2. Add those features to our exogenous variables `X`. Use `pd.merge` function, `left_index` and `right_index` options may be useful.\n",
    "3. Train the ARIMAX model (or SARIMAX, if you detected seasonality before). Use the best transformation from the previous exercise.\n",
    "4. Comment on the results, and compare them to the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34c7b9-1a2b-4545-bff7-d4d7e3c2d975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89244789c7b9f8fd",
   "metadata": {},
   "source": [
    "// comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129bd0f-71e9-440c-b33c-73bb157f101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
